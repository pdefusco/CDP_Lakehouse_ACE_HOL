{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "671b4d79-6642-4574-8083-e20c7566537d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e2e2392a-ee3a-4083-bf40-58bd0de663ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "username = \"052123\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19a8c1b9-f896-4980-9c8f-c2bdbc031433",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting spark.hadoop.yarn.resourcemanager.principal to pauldefusco\n"
     ]
    }
   ],
   "source": [
    "# Spark Dynamic Allocation is off by default but CML Overrides that to true\n",
    "# Disabling DA for demo purposes. We will use it later.\n",
    "\n",
    "# Spark AQE is on by default in Spark 3.2+\n",
    "# Disabling AQE for demo purposes. We will use it later.\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"PythonSQL\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.s3guard.ddb.region\",\"us-east-2\")\\\n",
    "    .config(\"spark.yarn.access.hadoopFileSystems\",\"s3a://go01-demo\")\\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"false\")\\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"false\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b04531d-7e92-4e18-a241-5d8b0138c924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.dynamicAllocation.enabled', 'false'),\n",
       " ('spark.eventLog.enabled', 'true'),\n",
       " ('spark.ui.proxyRedirectUri',\n",
       "  'https://spark-hgx2s40gc88b8mgh.ml-4c5feac0-3ec.go01-dem.ylcu-atmi.cloudera.site'),\n",
       " ('spark.hadoop.fs.s3a.s3guard.ddb.region', 'us-east-2'),\n",
       " ('spark.network.crypto.enabled', 'true'),\n",
       " ('spark.kubernetes.driver.pod.name', 'hgx2s40gc88b8mgh'),\n",
       " ('spark.kerberos.renewal.credentials', 'ccache'),\n",
       " ('spark.dynamicAllocation.maxExecutors', '49'),\n",
       " ('spark.eventLog.dir', 'file:///sparkeventlogs'),\n",
       " ('spark.hadoop.yarn.resourcemanager.principal', 'pauldefusco'),\n",
       " ('spark.kubernetes.driver.annotation.cluster-autoscaler.kubernetes.io/safe-to-evict',\n",
       "  'false'),\n",
       " ('spark.ui.port', '20049'),\n",
       " ('spark.kubernetes.executor.annotation.cluster-autoscaler.kubernetes.io/safe-to-evict',\n",
       "  'false'),\n",
       " ('spark.driver.memory', '3051m'),\n",
       " ('spark.io.encryption.enabled', 'true'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.driver.bindAddress', '100.100.92.161'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.yarn.access.hadoopFileSystems', 's3a://go01-demo'),\n",
       " ('spark.sql.adaptive.enabled', 'false'),\n",
       " ('spark.master', 'k8s://https://172.20.0.1:443'),\n",
       " ('spark.kubernetes.executor.podTemplateFile', '/tmp/spark-executor.json'),\n",
       " ('spark.kubernetes.container.image',\n",
       "  'docker.repository.cloudera.com/cloudera/cdsw/ml-runtime-jupyterlab-python3.7-standard:2022.11.1-b2'),\n",
       " ('spark.dynamicAllocation.shuffleTracking.enabled', 'true'),\n",
       " ('spark.kubernetes.namespace', 'mlx-user-16'),\n",
       " ('spark.app.id', 'spark-application-1684700144589'),\n",
       " ('spark.kubernetes.executor.config.dir', '/var/spark/conf'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  's3a://go01-demo/warehouse/tablespace/external/hive'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.name', 'PythonSQL'),\n",
       " ('spark.yarn.rmProxy.enabled', 'false'),\n",
       " ('spark.kubernetes.executor.podNamePrefix', 'cdsw-hgx2s40gc88b8mgh'),\n",
       " ('spark.driver.host', '100.100.92.161'),\n",
       " ('spark.driver.port', '45145'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.ui.allowFramingFrom',\n",
       "  'https://ml-4c5feac0-3ec.go01-dem.ylcu-atmi.cloudera.site'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.app.startTime', '1684700142469'),\n",
       " ('spark.deploy.mode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.authenticate', 'true')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223a9484-4b82-4c44-a450-6d18166337c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2eb15dfb-0f6b-4ded-ab46-d6f30a20031e",
   "metadata": {},
   "source": [
    "https://mageswaran1989.medium.com/spark-jargon-for-starters-af1fd8117ada#:~:text=A%20worker%20node%20can%20be,sufficient%20CPU%2C%20Memory%20and%20Storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0ed3f5-c9df-4c05-bb54-802df5eb4d03",
   "metadata": {},
   "source": [
    "https://www.linkedin.com/pulse/just-enough-spark-core-concepts-revisited-deepak-rajak/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac2271b-cb2d-46f2-82bc-9c0e95c882c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8617c13f-1c44-4b3d-adb5-0e18907791af",
   "metadata": {},
   "source": [
    "#### Spark Basic Architecture\n",
    "A cluster, or group of machines, pools the resources of many machines together allowing us to use all the cumulative resources as if they were one. Now a group of machines sitting somewhere alone is not powerful, you need a framework to coordinate work across them. Spark is a tailor-made engine exactly for this, managing and coordinating the execution of tasks on data across a cluster of computers. \n",
    "\n",
    "The cluster of machines that Spark will leverage to execute tasks will be managed by a cluster manager like Spark’s Standalone cluster manager, YARN - Yet Another Resource Negotiator, Kubernetes. We then submit Spark Applications to these cluster managers which will grant resources to our application so that we can complete our work. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5889a3-5136-4bb2-954b-a7c09a512ee9",
   "metadata": {},
   "source": [
    "#### Spark Application\n",
    "Spark Applications consist of a driver process and a set of executor processes. In the illustration we see above, our driver is on the left and four executors on the right."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b682b5-bec8-421c-8ae0-c16fd8f3108d",
   "metadata": {},
   "source": [
    "#### Spark Worker Nodes\n",
    "The worker nodes contain the executors which are responsible for actually carrying out the work that the driver assigns them. The Cluster Manager controls physical machines and allocates resources to the Spark Application. There can be multiple Spark Applications running on a cluster at the same time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f284f3-5c4b-4ae0-bef1-5605b8e6becd",
   "metadata": {},
   "source": [
    "#### What is a JVM?\n",
    "The JVM manages system memory and provides a portable execution environment for Java-based applications \n",
    "\n",
    "Technical definition: The JVM is the specification for a software program that executes code and provides the runtime environment for that code. \n",
    "\n",
    "Everyday definition: The JVM is how we run our Java programs. We configure the JVM's settings and then rely on it to manage program resources during execution. \n",
    "\n",
    "The Java Virtual Machine (JVM) is a program whose purpose is to execute other programs. \n",
    "\n",
    "The JVM has two primary functions: \n",
    "\n",
    "To allow Java programs to run on any device or operating system (known as the \"Write once, run anywhere\" principle)\n",
    "To manage and optimize program memory "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1ee84f-aa22-44e8-b54e-531c2776f92c",
   "metadata": {},
   "source": [
    "#### What is a Driver?\n",
    "The driver process runs your main() functions , sits on the node in the cluster and is responsible for 3 main things:\n",
    "\n",
    "Maintaining information about the spark application. Its a heart of a spark application and maintains all the I information during the lifetime of the application.\n",
    "Responding to user’s program or input.\n",
    "Analyzing, distributing and scheduling work across the executors.\n",
    "\n",
    "When a Spark program is triggered (e.g. a cell in this notebook performing Spark transformations and actions) the Driver converts the user program into tasks and after that it schedules the tasks on the executors.\n",
    "\n",
    "Spark Application — -> Driver — -> List of Tasks — -> Scheduler — -> Executors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e071d824-038f-4d6c-b5e6-1c5ebed44e08",
   "metadata": {},
   "source": [
    "#### What is an Executor?\n",
    "The executors are responsible for carrying out the work that the driver assigns them.\n",
    "\n",
    "Executors are worker nodes’ processes in charge of running individual tasks in a given Spark job. They are launched at the beginning of a Spark application and typically run for the entire lifetime of an application. Once they have run the task they send the results to the driver. They also provide in-memory storage for RDDs that are cached by user programs through Block Manager.\n",
    "\n",
    "When executors are started they register themselves with the driver and from so on they communicate directly. The workers are in charge of communicating the cluster manager the availability of their resources.\n",
    "\n",
    "* Execute code assigned to it by the driver.\n",
    "* Reporting the state of the computation on that executor back to driver."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6bc4c4-aa50-4a0a-bf94-f7dbabc86632",
   "metadata": {},
   "source": [
    "#### What is Cores/Slots/Threads?\n",
    "Spark parallelizes at two levels. One is the splitting the work among executors. The other is the slot. Each executor has a number of slots. Each slot can be assigned a Task. \n",
    "\n",
    "The JVM is naturally multithreaded, but a single JVM, such as our Driver, has a finite upper limit.\n",
    "By creating Tasks, the Driver can assign units of work to Slots on each Executor for parallel execution.\n",
    "Additionally, the Driver must also decide how to partition the data so that it can be distributed for parallel processing (see below).\n",
    "Consequently, the Driver is assigning a Partition of data to each task - in this way each Task knows which piece of data it is to process.\n",
    "Once started, each Task will fetch from the original data source (e.g. An Azure Storage Account) the Partition of data assigned to it.\n",
    "You can set the number of task slots to a value two or three times (i.e. to a multiple of) the number of CPU cores. Although these task slots are often referred to as CPU cores in Spark, they’re implemented as threads that work on a physical core's thread and don’t need to correspond to the number of physical CPU cores on the machine (since different CPU manufacturer's can architect multi-threaded chips differently). \n",
    "\n",
    "In other words:\n",
    "\n",
    "All processors of today have multiple cores (e.g. 1 CPU = 8 Cores)\n",
    "Most processors of today are multi-threaded (e.g. 1 Core = 2 Threads, 8 cores = 16 Threads)\n",
    "A Spark Task runs on a Slot. 1 Thread is capable of doing 1 Task at a time. To make use of all our threads on the CPU, we cleverly assign the number of Slots to correspond to a multiple of the number of Cores (which translates to multiple Threads).\n",
    "By doing this, after the Driver breaks down a given command into Tasks and Partitions, which are tailor-made to fit our particular Cluster Configuration (say 4 nodes - 1 driver and 3 executors, 8 cores per node, 2 threads per core). By using our Clusters at maximum efficiency like this (utilizing all available threads), we can get our massive command executed as fast as possible (given our Cluster in this case, 3*8*2 Threads --> 48 Tasks, 48 Partitions - i.e. 1 Partition per Task)\n",
    "If we don't do then even with a 100 executor cluster, the entire burden would go to 1 executor, and the other 99 will be sitting idle - i.e. slow execution.\n",
    "if we foolishly assign 49 Tasks and 49 Partitions, the first pass would execute 48 Tasks in parallel across the executors cores (say in 10 minutes), then that 1 remaining Task in the next pass will execute on 1 core for another 10 minutes, while the rest of our 47 cores are sitting idle - meaning the whole job will take double the time at 20 minutes. This is obviously an inefficient use of our available resources, and could rather be fixed by setting the number of tasks/partitions to a multiple of the number of cores we have (in this setup - 48, 96 etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8a00ee-67b9-42b9-a1ed-d77046941c58",
   "metadata": {},
   "source": [
    "#### What is a Partition?\n",
    "In order to allow every executor to perform work in parallel, Spark breaks up the data into chunks, called partitions.\n",
    "\n",
    "A partition is a collection of rows that sit on one physical machine in our cluster. A DataFrame’s partitions represent how the data is physically distributed across your cluster of machines during execution:\n",
    "\n",
    "If you have one partition, Spark will only have a parallelism of one, even if you have thousands of executors. \n",
    "If you have many partitions, but only one executor, Spark will still only have a parallelism of one because there is only one computation resource. \n",
    "An important thing to note is that with DataFrames, we do not (for the most part) manipulate partitions manually (on an individual basis). We simply specify high level transformations of data in the physical partitions and Spark determines how this work will actually execute on the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c54dc18-2b65-42ce-92e7-cd947378c255",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7007538-1917-422a-b047-8ee0d6f932d8",
   "metadata": {},
   "source": [
    "#### What happens when you launch a Spark Session\n",
    "When you create the SparkContext, each worker starts an executor. This is a separate process (JVM), and it loads your jar too. The executors connect back to your driver program. Now the driver can send them commands, like flatMap, map and reduceByKey in your example. When the driver quits, the executors shut down.\n",
    "\n",
    "RDDs are sort of like big arrays that are split into partitions, and each executor can hold some of these partitions.\n",
    "\n",
    "A task is a command sent from the driver to an executor by serializing your Function object. The executor deserializes the command (this is possible because it has loaded your jar), and executes it on a partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99114de-ec5a-41ef-8248-3f49cf519e28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7d1b129-4258-4fe4-a78e-f3bcbe00d81b",
   "metadata": {},
   "source": [
    "#### What is a DAG?\n",
    "Directed Acyclic Graph ( DAG ) in Apache Spark is a set of Vertices and Edges, where vertices represent the RDDs and the edges represent the Operation to be applied on RDDs.\n",
    "\n",
    "DAGScheduler is the scheduling layer of Apache Spark that implements stage-oriented scheduling. It transforms a logical execution plan to a physical execution plan (using stages).\n",
    "\n",
    "After an action has been called, SparkContext hands over a logical plan to DAGScheduler that it in turn translates to a set of stages that are submitted as a set of tasks for execution.\n",
    "\n",
    "The fundamental concepts of DAGScheduler are jobs and stages that it tracks through internal registries and counters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9b3ece-60ad-438c-bc6c-2e4e01fbec18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e7a1515-dcab-4af6-98a4-f2a0ebb2cd32",
   "metadata": {},
   "source": [
    "#### What is a Job?\n",
    "A Job is a sequence of stages, triggered by an action such as count(), collect(), read() or write(). \n",
    "\n",
    "Each parallelized action is referred to as a Job.\n",
    "The results of each Job (parallelized/distributed action) is returned to the Driver from the Executor.\n",
    "Depending on the work required, multiple Jobs will be required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64070f44-c4ec-4109-b02d-b89694edeb55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0db0876d-2e8e-4082-ba8a-f80ec6f6cbd6",
   "metadata": {},
   "source": [
    "#### What is a Stage?\n",
    "Each job that gets divided into smaller sets of tasks is a stage.\n",
    "\n",
    "A Stage is a sequence of Tasks that can all be run together - i.e. in parallel - without a shuffle. For example: using \".read\" to read a file from disk, then runnning \".filter\" can be done without a shuffle, so it can fit in a single stage. The number of Tasks in a Stage also depends upon the number of Partitions your datasets have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5401d13-3953-43be-a701-5435834335bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78218acf-9812-4140-a2c7-f19be8d4e0c4",
   "metadata": {},
   "source": [
    "#### What is a Task?\n",
    "A task is a unit of work that is sent to the executor. Each stage has some tasks, one task per partition. The same task is done over different partitions of the RDD.\n",
    "\n",
    "In the example of Stages above, each Step is a Task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b527eb8-ae40-4d19-8541-4192f0d1df6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "314d911d-2e07-4dd7-abab-046f8420158c",
   "metadata": {},
   "source": [
    "#### What is a Caching?\n",
    "In applications that reuse the same datasets over and over, one of the most useful optimizations is caching. Caching will place a DataFrame or table into temporary storage across the executors in your cluster and make subsequent reads faster. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5631bd60-3220-4bd4-8ef0-87bf567fe3b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b9dd788-50dd-4820-b50e-bee91a7a6224",
   "metadata": {},
   "source": [
    "#### What is a Shuffling?\n",
    "A Shuffle refers to an operation where data is re-partitioned across a Cluster - i.e. when data needs to move between executors.\n",
    "\n",
    "join and any operation that ends with ByKey will trigger a Shuffle. It is a costly operation because a lot of data can be sent via the network.\n",
    "\n",
    "For example, to group by color, it will serve us best if...\n",
    "\n",
    "All the reds are in one partitions\n",
    "All the blues are in a second partition\n",
    "All the greens are in a third\n",
    "From there we can easily sum/count/average all of the reds, blues, and greens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ed08e4-e1e4-4795-9ca6-b4766c92f490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4ce92bb-da19-4ad7-8d14-32d33d04743c",
   "metadata": {},
   "source": [
    "#### What is a Partitioning?\n",
    "A Partition is a logical chunk of your DataFrame\n",
    "\n",
    "Data is split into Partitions so that each Executor can operate on a single part, enabling parallelization.\n",
    "\n",
    "It can be processed by a single Executor core/thread.\n",
    "\n",
    "For example: If you have 4 data partitions and you have 4 executor cores/threads, you can process everything in parallel, in a single pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802a4080-5d0b-4762-ae60-c040f18896da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2eee2167-1a1f-4b04-a433-e95718e732f5",
   "metadata": {},
   "source": [
    "#### Transformations\n",
    "\n",
    "In Spark, the core data structures are immutable meaning they cannot be changed once created. In order to \"change\" a DataFrame you will have to instruct Spark how you would like to modify the DataFrame you have into the one that you want. These instructions are called transformations. Let’s perform a simple transformation to find all even numbers in our currentDataFrame. Examples – Select, Filter, GroupBy, Join, Union, Partition etc\n",
    "\n",
    "Actions\n",
    "\n",
    "Transformations allow us to build up our logical transformation plan. To trigger the computation, we run an action. An action instructs Spark to compute a result from a series of transformations. The simplest action is count which gives us the total number of records in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64578a5d-0f46-4b39-b09d-16d5fedf5148",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e56042a8-1e02-48de-8b82-c5947f9beb0e",
   "metadata": {},
   "source": [
    "#### Narrow Transformations Vs Wide Transformations\n",
    "There are two types of transformations: Narrow and Wide.\n",
    "\n",
    "For narrow transformations, the data required to compute the records in a single partition reside in at most one partition of the parent dataset. \n",
    "\n",
    "Examples include:\n",
    "\n",
    "filter(..)\n",
    "drop(..)\n",
    "coalesce()\n",
    "For wide transformations, the data required to compute the records in a single partition may reside in many partitions of the parent dataset. \n",
    "\n",
    "Examples include:\n",
    "\n",
    "distinct()\n",
    "groupBy(..).sum()\n",
    "repartition(n)\n",
    "Remember, spark partitions are collections of rows that sit on physical machines in the cluster. Narrow transformations mean that work can be computed and reported back to the executor without changing the way data is partitioned over the system. Wide transformations require that data be redistributed over the system. This is called a shuffle. \n",
    "\n",
    "Shuffles are triggered when data needs to move between executors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fa2a3a-3748-44dc-856a-d97a5ec02667",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0db4288-f8f2-4873-8682-6c89d8433201",
   "metadata": {},
   "source": [
    "### Application Execution Flow\n",
    "With this in mind, when you submit an application to the cluster with spark-submit this is what happens internally:\n",
    "\n",
    "An application starts and instantiates a SparkContext/SparkSession instance (and it is only then when you can call the application a driver).\n",
    "The driver program ask for resources to the cluster manager to launch executors.\n",
    "The cluster manager launches executors.\n",
    "The driver process runs through the user application. Depending on the actions and transformations over RDDs task are sent to executors.\n",
    "Executors run the tasks and save the results.\n",
    "If any worker crashes, its tasks will be sent to different executors to be processed again.\n",
    "\n",
    "Spark automatically deals with failed or slow machines by re-executing failed or slow tasks. For example, if the node running a partition of a map() operation crashes, Spark will rerun it on another node; and even if the node does not crash but is simply much slower than other nodes, Spark can preemptively launch a “speculative” copy of the task on another node, and take its result if that finishes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3d61d7-b2a9-4f61-9592-f7fde0615a29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d7f998-1b5a-4782-bfc4-2fbc0b9f8364",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55b3c799-7054-4fc5-8cbc-220e73fb1756",
   "metadata": {},
   "source": [
    "#### Dataframes vs Datasets vs RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb979089-3927-4b66-8858-f0d3d4ae709a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32ed6ca8-c975-40c0-b333-cb5fac2bd05d",
   "metadata": {},
   "source": [
    "Initially, in 2011 in they came up with the concept of RDDs, then in 2013 with Dataframes and later in 2015 with the concept of Datasets. None of them has been depreciated, we can still use all of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f3452f-2420-4c32-afdd-a80eb520d40b",
   "metadata": {},
   "source": [
    "#### What are RDDs?\n",
    "RDDs or Resilient Distributed Datasets is the fundamental data structure of the Spark. It is the collection of objects which is capable of storing the data partitioned across the multiple nodes of the cluster and also allows them to do processing in parallel.\n",
    "\n",
    "It is fault-tolerant if you perform multiple transformations on the RDD and then due to any reason any node fails. The RDD, in that case, is capable of recovering automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0867e13-8c2b-43b3-8412-c8fd0b39b6ca",
   "metadata": {},
   "source": [
    "#### What are Dataframes?\n",
    "It was introduced first in Spark version 1.3 to overcome the limitations of the Spark RDD. Spark Dataframes are the distributed collection of the data points, but here, the data is organized into the named columns. They allow developers to debug the code during the runtime which was not allowed with the RDDs.\n",
    "\n",
    "Dataframes can read and write the data into various formats like CSV, JSON, AVRO, HDFS, and HIVE tables. It is already optimized to process large datasets for most of the pre-processing tasks so that we do not need to write complex functions on our own.\n",
    "\n",
    "It uses a catalyst optimizer for optimization purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3320af7b-a363-4862-bcaa-16deecfb0e65",
   "metadata": {},
   "source": [
    "#### What are Datasets?\n",
    "Spark Datasets is an extension of Dataframes API with the benefits of both RDDs and the Datasets. It is fast as well as provides a type-safe interface. Type safety means that the compiler will validate the data types of all the columns in the dataset while compilation only and will throw an error if there is any mismatch in the data types.\n",
    "\n",
    "Users of RDD will find it somewhat similar to code but it is faster than RDDs. It can efficiently process both structured and unstructured data.\n",
    "\n",
    "We cannot create Spark Datasets in Python yet. The dataset API is available only in Scala and Java only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafc3de1-a7c5-4309-85f3-56a97e59fb90",
   "metadata": {},
   "source": [
    "Q1. What is difference between RDD and DataFrame?\n",
    "A. RDD (Resilient Distributed Dataset) is the fundamental data structure in Apache Spark, representing an immutable distributed collection of objects. It offers low-level operations and lacks optimization benefits provided by higher-level abstractions.\n",
    "DataFrames, on the other hand, are higher-level abstractions built on top of RDDs. They provide structured and optimized distributed data processing with a schema, supporting SQL-like queries, and various optimizations for better performance.\n",
    "\n",
    "Q2. When should I use RDD over DataFrame?\n",
    "A. RDDs are useful in scenarios where you require low-level control over data and need to perform complex custom transformations or need to access RDD-specific operations not available in DataFrames. Additionally, RDDs are suitable when working with unstructured data or when integrating with non-Spark libraries that expect RDDs as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26736a9a-090f-43d9-9e0d-dd5203e88c3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707ee729-1ee8-424c-9ca4-36b8e458f2b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18637f0d-8d38-4ab7-bc20-3531d152ad9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f524a9af-4bab-4857-b4d7-ef573d7506cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07d4ba66-385c-4742-b441-2bacb50766d9",
   "metadata": {},
   "source": [
    "### FAQ's"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c34270-dc7c-4fe6-a0a8-b8dd9cda1d00",
   "metadata": {},
   "source": [
    "Does every worker instance hold an executor for specific application (which manages storage, task) or one worker node holds one executor?\n",
    "\n",
    "Workers hold many executors, for many applications. One application has executors on many workers\n",
    "\n",
    "A worker node can be holding multiple executors (processes) if it has sufficient CPU, Memory and Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a24386-eadd-4562-bc53-6e49fba32440",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8986e146-4498-433f-9750-c6180287921c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65d6a1e-62cd-48a6-b515-1c97a4c6108f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2eaf6516-78e6-4233-aac2-146fc7147f42",
   "metadata": {},
   "source": [
    "### Working with More Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "847955d4-03d5-435b-8600-f145edaebc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Version : VersionInfo(major='0', minor='2', patch='1', release='', build='')\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import LongType, IntegerType, StringType\n",
    "\n",
    "import dbldatagen as dg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2ddfe4e-575b-46e2-9749-6d5b35b6827d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_df(row_count = 100000, unique_vals=100000):\n",
    "\n",
    "    #shuffle_partitions_requested = 8\n",
    "    \n",
    "    #spark.conf.set(\"spark.sql.shuffle.partitions\", shuffle_partitions_requested)\n",
    "\n",
    "    country_codes = [\n",
    "        \"CN\", \"US\", \"FR\", \"CA\", \"IN\", \"JM\", \"IE\", \"PK\", \"GB\", \"IL\", \"AU\", \n",
    "        \"SG\", \"ES\", \"GE\", \"MX\", \"ET\", \"SA\", \"LB\", \"NL\", \"IT\"\n",
    "    ]\n",
    "    #country_weights = [\n",
    "    #    1300, 365, 67, 38, 1300, 3, 7, 212, 67, 9, 25, 6, 47, 83, \n",
    "    #    126, 109, 58, 8, 17,\n",
    "    #]\n",
    "\n",
    "    manufacturers = [\n",
    "        \"Delta corp\", \"Xyzzy Inc.\", \"Lakehouse Ltd\", \"Acme Corp\", \"Embanks Devices\",\n",
    "    ]\n",
    "\n",
    "    lines = [\"delta\", \"xyzzy\", \"lakehouse\", \"gadget\", \"droid\"]\n",
    "\n",
    "    testDataSpec = (\n",
    "        dg.DataGenerator(spark, name=\"device_data_set\", rows=row_count) #,partitions=partitions_num)\n",
    "        .withIdOutput()\n",
    "        # we'll use hash of the base field to generate the ids to\n",
    "        # avoid a simple incrementing sequence\n",
    "        .withColumn(\"internal_device_id\", \"long\", minValue=0x1000000000000, \n",
    "                    uniqueValues=unique_vals, omit=True, baseColumnType=\"hash\",\n",
    "        )\n",
    "        # note for format strings, we must use \"%lx\" not \"%x\" as the\n",
    "        # underlying value is a long\n",
    "        .withColumn(\n",
    "            \"device_id\", \"string\", format=\"0x%013x\", baseColumn=\"internal_device_id\"\n",
    "        )\n",
    "        # the device / user attributes will be the same for the same device id\n",
    "        # so lets use the internal device id as the base column for these attribute\n",
    "        .withColumn(\"country\", \"string\", values=country_codes, #weights=country_weights, \n",
    "                    baseColumn=\"internal_device_id\")\n",
    "        .withColumn(\"manufacturer\", \"string\", values=manufacturers, \n",
    "                    baseColumn=\"internal_device_id\", )\n",
    "        # use omit = True if you don't want a column to appear in the final output\n",
    "        # but just want to use it as part of generation of another column\n",
    "        .withColumn(\"line\", \"string\", values=lines, baseColumn=\"manufacturer\", \n",
    "                    baseColumnType=\"hash\", omit=True )\n",
    "        .withColumn(\"model_ser\", \"integer\", minValue=1, maxValue=11, baseColumn=\"device_id\", \n",
    "                    baseColumnType=\"hash\", omit=True, )\n",
    "        .withColumn(\"model_line\", \"string\", expr=\"concat(line, '#', model_ser)\", \n",
    "                    baseColumn=[\"line\", \"model_ser\"] )\n",
    "        .withColumn(\"event_type\", \"string\", \n",
    "                    values=[\"activation\", \"deactivation\", \"plan change\", \"telecoms activity\", \n",
    "                            \"internet activity\", \"device error\", ],\n",
    "                    random=True)\n",
    "        .withColumn(\"event_ts\", \"timestamp\", begin=\"2020-01-01 01:00:00\", \n",
    "                    end=\"2020-12-31 23:59:00\", \n",
    "                    interval=\"1 minute\", random=True )\n",
    "    )\n",
    "\n",
    "    dfTestData = testDataSpec.build()\n",
    "\n",
    "    display(dfTestData)\n",
    "    \n",
    "    return dfTestData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02bb842-35c1-4474-8a77-f66dd71ca06e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "214c5a97-131c-442d-8ad6-93a2d828b278",
   "metadata": {},
   "source": [
    "When you are running Spark application in yarn or any cluster manager, the default length/size of partitions RDD/DataFrame/Dataset are created with the total number of cores on all executor nodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dde082-aa79-4e76-b4a5-b12873c7279a",
   "metadata": {},
   "source": [
    "Spark automatically sets the number of “map” tasks to run on each file according to its size (though you can control it through optional parameters to SparkContext.textFile, etc), and for distributed “reduce” operations, such as groupByKey and reduceByKey, it uses the largest parent RDD’s number of partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "40cf6dfa-ca4b-4bdf-aeca-c03045e6431d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, device_id: string, country: string, manufacturer: string, model_line: string, event_type: string, event_ts: timestamp]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = generate_df(row_count = 100000, unique_vals=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab173d9a-b29f-46df-95ce-73e9449e9309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3ebb4dfa-6152-4fd5-8cf9-66d286e781a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.mode(\"overwrite\").saveAsTable('SPARK.IOT_DATA_{}'.format(username), format=\"parquet\") #partitionBy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c45c4332-b9dc-4867-8d53-99b4f27361b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3a://go01-demo/warehouse/tablespace/external/hive'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.warehouse.dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "17e7eeeb-c892-48a1-b56f-334ede1ff435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'docker.repository.cloudera.com/cloudera/cdsw/ml-runtime-jupyterlab-python3.7-standard:2022.11.1-b2'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.kubernetes.container.image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d62be26-70e0-4495-b008-a14dc6e7e085",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d129baba-44bc-44c0-a6aa-e3e6612a3453",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f51b679-093f-46f0-8f22-65999fa0d31d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0736484c-63e7-43c3-8c16-4cb216b8e49b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42d81a6-62d2-45fb-b13e-3562eaa67279",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8f3b0a-ed3a-4eb1-b39b-4734e3cce242",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e24c304-aa8f-4b19-9278-c1f41a9e9dd8",
   "metadata": {},
   "source": [
    "Turn AQE and DA off. Run spark jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13d3d1b-d65d-4786-bd9e-93ffb5370ff3",
   "metadata": {},
   "source": [
    "Turn DA On and do section on it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32241ee9-0979-486d-81f3-a358eb76d1be",
   "metadata": {},
   "source": [
    "Turn AQE On and do section on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ed4000-08ce-4348-83c5-32db071032ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dad39725-b9d6-47a1-8c91-de16d20041d0",
   "metadata": {},
   "source": [
    "#### Spark Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b43422-dab4-45dc-8f5b-ad50f46cd5a4",
   "metadata": {},
   "source": [
    "Spark Merge Into Workaround"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d0f8f8-5e39-4ee5-8799-88a24ddebb99",
   "metadata": {},
   "source": [
    "Spark Schema Evolution Workaround"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc7431a-5d4a-4910-9654-b19cd6aea6f5",
   "metadata": {},
   "source": [
    "Spark Partition Evolution Workaround"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208ea9e5-3405-4752-9708-d8467d479e74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
