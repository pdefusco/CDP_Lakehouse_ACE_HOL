{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ea81c34-2d55-4646-8d4f-40ad84e842ff",
   "metadata": {},
   "source": [
    "### Hadoop and Hive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21020d1d-b972-4a36-bd9c-e405a1c31959",
   "metadata": {},
   "source": [
    "Hadoop was first released by Apache in 2011 as Version 1.0.0, which only contained HDFS and MapReduce. Hadoop was designed as both a computing (MapReduce) and storage (HDFS) platform from the very beginning. With the increasing need for big data analysis, Hadoop attracts lots of other software to resolve big data questions and merges into a Hadoop-centric big data ecosystem. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef583e4e-323a-4227-bffb-006f28343af4",
   "metadata": {},
   "source": [
    "Hive is a standard for SQL queries over petabytes of data in Hadoop. It provides SQL-like access to data in HDFS, enabling Hadoop to be used as a data warehouse. The Hive Query Language (HQL) has similar semantics and functions as standard SQL in the relational database, so that experienced database analysts can easily get their hands on it. Hive's query language can run on different computing engines, such as MapReduce, Tez, and Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62d61b1-8427-4d46-9108-3657a81c2d3d",
   "metadata": {},
   "source": [
    "Hive's metadata structure provides a high-level, table-like structure on top of HDFS. It supports three main data structures, tables, partitions, and buckets. The tables correspond to HDFS directories and can be divided into partitions, where data files can be divided into buckets. Hive's metadata structure is usually the Schema of the Schema-on-Read concept on Hadoop, which means you do not have to define the schema in Hive before you store data in HDFS. Applying Hive metadata after storing data brings more flexibility and efficiency to your data work. The popularity of Hive's metadata makes it the de facto way to describe big data and is used by many tools in the big data ecosystem. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8620040-d84c-454a-8b98-4530dd5e53ae",
   "metadata": {},
   "source": [
    "Here are some highlights of Hive that we can keep in mind moving forward:\n",
    "\n",
    "    Hive provides a simple and optimized query model with less coding than MapReduce\n",
    "    HQL and SQL have a similar syntax\n",
    "    Hive's query response time is typically much faster than others on the same volume of big datasets\n",
    "    Hive supports running on different computing frameworks\n",
    "    Hive supports ad hoc querying data on HDFS and HBase\n",
    "    Hive supports user-defined java/scala functions, scripts, and procedure languages to extend its functionality\n",
    "    Matured JDBC and ODBC drivers allow many applications to pull Hive data for seamless reporting\n",
    "    Hive allows users to read data in arbitrary formats, using SerDes and Input/Output formats\n",
    "    Hive is a stable and reliable batch-processing tool, which is production-ready for a long time\n",
    "    Hive has a well-defined architecture for metadata management, authentication, and query optimizations\n",
    "    There is a big community of practitioners and developers working on and using Hive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4d989e-2974-493c-a43a-a61bc87491f8",
   "metadata": {},
   "source": [
    "Cloudera Data Warehouse (CDW) Data Service is a containerized application for creating highly performant, independent, self-service data warehouses in the cloud which can be scaled dynamically and upgraded independently. Learn more about the service architecture, and how CDW enables data practitioners and IT administrators to achieve their goals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c36ad1-d926-4ad9-a07b-935f53295d34",
   "metadata": {},
   "source": [
    "Cloudera Machine Learning now offers Snippet to connect to Data Sources available within the CDP Environment. Administrators can configure custom Spark, Hive or Impala Virtual Warehouse data connections manually or they can use CMLâ€™s features to autodetect and configure all connections from the same CDP Environment. Data Scientists can then access the preconfigured Data Connections from their ML Projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48ad0bae-0449-4c6a-ba68-bcda3bbbc797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            database_name\n",
      "0             01_car_data\n",
      "1               01_car_dw\n",
      "2          adash_car_data\n",
      "3                 airline\n",
      "4              airline_dw\n",
      "..                    ...\n",
      "143  user_test_3_car_data\n",
      "144                vademo\n",
      "145         worldwidebank\n",
      "146           yanliu_test\n",
      "147     yingchen_car_data\n",
      "\n",
      "[148 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import cml.data_v1 as cmldata\n",
    "\n",
    "CONNECTION_NAME = \"default-hive-aws\"\n",
    "conn = cmldata.get_connection(CONNECTION_NAME)\n",
    "\n",
    "## Sample Usage to get pandas data frame\n",
    "EXAMPLE_SQL_QUERY = \"show databases\"\n",
    "dataframe = conn.get_pandas_dataframe(EXAMPLE_SQL_QUERY)\n",
    "print(dataframe)\n",
    "\n",
    "## Other Usage Notes:\n",
    "\n",
    "## Alternate Sample Usage to provide different credentials as optional parameters\n",
    "#conn = cmldata.get_connection(\n",
    "#    CONNECTION_NAME, {\"USERNAME\": \"someuser\", \"PASSWORD\": \"somepassword\"}\n",
    "#)\n",
    "\n",
    "## Alternate Sample Usage to get DB API Connection interface\n",
    "#db_conn = conn.get_base_connection()\n",
    "\n",
    "## Alternate Sample Usage to get DB API Cursor interface\n",
    "#db_cursor = conn.get_cursor()\n",
    "#db_cursor.execute(EXAMPLE_SQL_QUERY)\n",
    "#for row in db_cursor:\n",
    "#  print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f900a59-d7b6-45e7-90a3-0c6909fd7566",
   "metadata": {},
   "source": [
    "### Hive Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e929096d-922f-4154-9b90-146e480fde02",
   "metadata": {},
   "source": [
    "The concept of a table in Hive is very similar to the table in the relational database. Each table maps to a directory, which is under /user/hive/warehouse by default in HDFS. For example, /user/hive/warehouse/employee is created for the employee table. All the data in the table is stored in this hive user-manageable directory (full permission). This kind of table is called an internal, or managed, table. When data is already stored in HDFS, an external table can be created to describe the data. It is called external because the data in the external table is specified in the LOCATION property rather than the default warehouse directory. When keeping data in the internal tables, the table fully manages the data in it. When an internal table is dropped, its data is deleted together. However, when an external table is dropped, the data is not deleted. It is quite common to use external tables for source read-only data or sharing the processed data to data consumers giving customized HDFS locations. On the other hand, the internal table is often used as an intermediate table during data processing, since it is quite powerful and flexible when supported by HQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9c0a5b7-0404-44b7-8d59-0dfc12e4da00",
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_QUERY = \"SHOW TABLES '*customer*'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31677134-e509-4155-8a45-34e6dd210eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 tab_name\n",
      "0                        01_customer_data\n",
      "1        ca_customer_issues_per_month_csv\n",
      "2                  customer_daily_profile\n",
      "3                           customer_data\n",
      "4                           customer_demo\n",
      "5                 customer_profiles_table\n",
      "6                           customer_temp\n",
      "7        customer_terminal_profiles_table\n",
      "8   customer_terminal_profiles_table_test\n",
      "9                            customerdata\n",
      "10                         customerdata1g\n",
      "11                      customerdata_temp\n",
      "12                          customerdatag\n",
      "13                        customerrecords\n",
      "14                              customers\n",
      "15                             customers2\n",
      "16                            customers_2\n",
      "17                         customers_ofer\n",
      "18                          customers_srm\n",
      "19                   historical_customers\n",
      "20                olist_customers_dataset\n",
      "21                      ww_customers_data\n"
     ]
    }
   ],
   "source": [
    "dataframe = conn.get_pandas_dataframe(SQL_QUERY)\n",
    "print(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3246471e-dd2c-4fd3-a86d-56928e543e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_QUERY = \"SHOW TBLPROPERTIES customers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6212e363-ec8c-4e7a-ac2a-e2cf2bf0fb37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               prpt_name                                         prpt_value\n",
      "0                comment                                                   \n",
      "1        kudu.cluster_id                   f3b70651dc5f44eb9e17aef582e0689f\n",
      "2  kudu.master_addresses  real-time-datamart-master10.go01-dem.ylcu-atmi...\n",
      "3          kudu.table_id                   4e0f48639b8d4e4096f6d446129004a3\n",
      "4        kudu.table_name                                  default.customers\n",
      "5        storage_handler     org.apache.hadoop.hive.kudu.KuduStorageHandler\n",
      "6  transient_lastDdlTime                                         1658862739\n"
     ]
    }
   ],
   "source": [
    "dataframe = conn.get_pandas_dataframe(SQL_QUERY)\n",
    "print(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61f6245c-ff93-43a5-b28f-1c8f35a296c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_QUERY = \"SHOW CREATE TABLE customers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0d06e33-0b10-42ac-89ef-8b5a4fa51ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       createtab_stmt\n",
      "0                           CREATE TABLE `customers`(\n",
      "1                        `acc_id` string COMMENT '', \n",
      "2                        `f_name` string COMMENT '', \n",
      "3                        `l_name` string COMMENT '', \n",
      "4                         `email` string COMMENT '', \n",
      "5                        `gender` string COMMENT '', \n",
      "6                         `phone` string COMMENT '', \n",
      "7                           `card` string COMMENT '')\n",
      "8                                          COMMENT ''\n",
      "9                                   ROW FORMAT SERDE \n",
      "10                                                '' \n",
      "11                                         STORED BY \n",
      "12    'org.apache.hadoop.hive.kudu.KuduStorageHand...\n",
      "13                                                   \n",
      "14                                           LOCATION\n",
      "15    's3a://go01-demo/warehouse/tablespace/manage...\n",
      "16                                    TBLPROPERTIES (\n",
      "17    'kudu.cluster_id'='f3b70651dc5f44eb9e17aef58...\n",
      "18    'kudu.master_addresses'='real-time-datamart-...\n",
      "19    'kudu.table_id'='4e0f48639b8d4e4096f6d446129...\n",
      "20            'kudu.table_name'='default.customers', \n",
      "21              'transient_lastDdlTime'='1658862739')\n"
     ]
    }
   ],
   "source": [
    "dataframe = conn.get_pandas_dataframe(SQL_QUERY)\n",
    "print(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3de02eae-af42-4fe1-96c0-29aee3e87690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table Folder Location in Cloud Storage: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"  's3a://go01-demo/warehouse/tablespace/managed/hive/customers'\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Table Folder Location in Cloud Storage: \")\n",
    "dataframe.loc[15][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cd30b7-b971-4afb-abbd-00700712811f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e8e7a47-cdde-4579-b31b-238c11567495",
   "metadata": {},
   "source": [
    "#### Partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e870bf63-4f0b-41a9-99ff-e74e146d5a44",
   "metadata": {},
   "source": [
    "By default, a simple HQL query scans the whole table. This slows down the performance when querying a big table. This issue could be resolved by creating partitions, which are very similar to what's in the RDBMS. In Hive, each partition corresponds to a predefined partition column(s), which maps to subdirectories in the table's directory in HDFS. When the table gets queried, only the required partitions (directory) of data in the table are being read, so the I/O and time of the query is greatly reduced. Using partition is a very easy and effective way to improve performance in Hive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389bb055-dc33-432e-9a71-81718eabd335",
   "metadata": {},
   "source": [
    "The following is an example of partition creation in HQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed134ef-a2d6-4821-a2a2-04e2fa696926",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273d1431-49a9-4908-97b8-42260eb3adda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2d0ce28-29ff-40ed-9f24-2c4c0ce69472",
   "metadata": {},
   "source": [
    "You can only change the partition column data type. We cannot add/remove a column from partition columns. If we have to change the partition design, we must back up and recreate the table, and then migrate the data. In addition, we are NOT able to change a non-partition table to a partition table directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99f903e-1264-41fd-b2fa-7221c4eeb10d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9ccf93-f599-4d6b-bb30-e920df5fc8d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
