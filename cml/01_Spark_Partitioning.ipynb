{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "671b4d79-6642-4574-8083-e20c7566537d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e2e2392a-ee3a-4083-bf40-58bd0de663ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "username = \"052123\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf2575a-f5d2-4dd5-8ab2-374c0f1aabcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19a8c1b9-f896-4980-9c8f-c2bdbc031433",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting spark.hadoop.yarn.resourcemanager.principal to pauldefusco\n"
     ]
    }
   ],
   "source": [
    "# Spark Dynamic Allocation is off by default but CML Overrides that to true\n",
    "# Disabling DA for demo purposes. We will use it later.\n",
    "\n",
    "# Spark AQE is on by default in Spark 3.2+\n",
    "# Disabling AQE for demo purposes. We will use it later.\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"PythonSQL\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.s3guard.ddb.region\",\"us-east-2\")\\\n",
    "    .config(\"spark.yarn.access.hadoopFileSystems\",\"s3a://go01-demo\")\\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"false\")\\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"false\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b04531d-7e92-4e18-a241-5d8b0138c924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.dynamicAllocation.enabled', 'false'),\n",
       " ('spark.eventLog.enabled', 'true'),\n",
       " ('spark.ui.proxyRedirectUri',\n",
       "  'https://spark-hgx2s40gc88b8mgh.ml-4c5feac0-3ec.go01-dem.ylcu-atmi.cloudera.site'),\n",
       " ('spark.hadoop.fs.s3a.s3guard.ddb.region', 'us-east-2'),\n",
       " ('spark.network.crypto.enabled', 'true'),\n",
       " ('spark.kubernetes.driver.pod.name', 'hgx2s40gc88b8mgh'),\n",
       " ('spark.kerberos.renewal.credentials', 'ccache'),\n",
       " ('spark.dynamicAllocation.maxExecutors', '49'),\n",
       " ('spark.eventLog.dir', 'file:///sparkeventlogs'),\n",
       " ('spark.hadoop.yarn.resourcemanager.principal', 'pauldefusco'),\n",
       " ('spark.kubernetes.driver.annotation.cluster-autoscaler.kubernetes.io/safe-to-evict',\n",
       "  'false'),\n",
       " ('spark.ui.port', '20049'),\n",
       " ('spark.kubernetes.executor.annotation.cluster-autoscaler.kubernetes.io/safe-to-evict',\n",
       "  'false'),\n",
       " ('spark.driver.memory', '3051m'),\n",
       " ('spark.io.encryption.enabled', 'true'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.driver.bindAddress', '100.100.92.161'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.yarn.access.hadoopFileSystems', 's3a://go01-demo'),\n",
       " ('spark.sql.adaptive.enabled', 'false'),\n",
       " ('spark.master', 'k8s://https://172.20.0.1:443'),\n",
       " ('spark.kubernetes.executor.podTemplateFile', '/tmp/spark-executor.json'),\n",
       " ('spark.kubernetes.container.image',\n",
       "  'docker.repository.cloudera.com/cloudera/cdsw/ml-runtime-jupyterlab-python3.7-standard:2022.11.1-b2'),\n",
       " ('spark.dynamicAllocation.shuffleTracking.enabled', 'true'),\n",
       " ('spark.kubernetes.namespace', 'mlx-user-16'),\n",
       " ('spark.app.id', 'spark-application-1684700144589'),\n",
       " ('spark.kubernetes.executor.config.dir', '/var/spark/conf'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  's3a://go01-demo/warehouse/tablespace/external/hive'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.name', 'PythonSQL'),\n",
       " ('spark.yarn.rmProxy.enabled', 'false'),\n",
       " ('spark.kubernetes.executor.podNamePrefix', 'cdsw-hgx2s40gc88b8mgh'),\n",
       " ('spark.driver.host', '100.100.92.161'),\n",
       " ('spark.driver.port', '45145'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.ui.allowFramingFrom',\n",
       "  'https://ml-4c5feac0-3ec.go01-dem.ylcu-atmi.cloudera.site'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.app.startTime', '1684700142469'),\n",
       " ('spark.deploy.mode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.authenticate', 'true')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9724c3-c219-4d38-84b3-532cb42c0b86",
   "metadata": {},
   "source": [
    "#### Hive Metastore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabe3150-a688-459a-9829-b5e09b1bdf27",
   "metadata": {},
   "source": [
    "Spark SQL uses a Hive metastore to manage the metadata of persistent relational entities (e.g. databases, tables, columns, partitions) in a relational database (for fast access). A Hive metastore warehouse (aka spark-warehouse) is the directory where Spark SQL persists tables whereas a Hive metastore (aka metastore_db) is a relational database to manage the metadata of the persistent relational entities, e.g. databases, tables, columns, partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af97d6a-e5de-410a-bf59-64fbd5a76a82",
   "metadata": {},
   "source": [
    "#### Hive Warehouse Connector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addbd601-2428-4fc7-b0cd-f7cf237beed3",
   "metadata": {},
   "source": [
    "HWC is software for securely accessing Hive tables from Spark. You need to use the HWC if you want to access Hive managed tables from Spark. You explicitly use HWC by calling the HiveWarehouseConnector API to write to managed tables. You might use HWC without even realizing it. HWC implicitly reads tables when you run a Spark SQL query on a Hive managed table.\n",
    "\n",
    "You do not need HWC to read or write Hive external tables. You can use native Spark SQL. Spark tables will be tracked in the HMS.\n",
    "\n",
    "In this tutorial we will not use the HWC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0d0f2c0-66a3-4278-aadc-55a7ff7fb95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+\n",
      "|      catalog|namespace|\n",
      "+-------------+---------+\n",
      "|spark_catalog|  default|\n",
      "+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show catalog and database\n",
    "spark.sql(\"SHOW CURRENT NAMESPACE\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c45fb5d-7f1c-4c3d-b31e-c6727db26bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hive Session ID = cbd83497-a3c6-487d-a297-eaf925f12e0e\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new database\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS spark_catalog.spark\")\n",
    "spark.sql(\"USE spark_catalog.spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dec4542a-3225-495d-bdeb-f8cffa5b18b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+\n",
      "|      catalog|namespace|\n",
      "+-------------+---------+\n",
      "|spark_catalog|    spark|\n",
      "+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show catalog and database\n",
    "spark.sql(\"SHOW CURRENT NAMESPACE\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eb8a57-68ae-4325-adf8-0c50518761cd",
   "metadata": {},
   "source": [
    "#### Non Partitioned Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "985487fe-ec13-4e87-8d86-4e10a02e50ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS spark.non_partitioned_table\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS spark.non_partitioned_table\\\n",
    "            (id BIGINT, state STRING, country STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6d8446e-f121-49b8-8807-8a75554069a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"INSERT INTO spark.non_partitioned_table VALUES (1, 'CA', 'USA'),(2, 'CA', 'USA'),\\\n",
    "                    (3, 'AZ', 'USA'),\\\n",
    "                    (4, 'ON', 'CAN'),\\\n",
    "                    (5, 'AL', 'CAN')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb1a306-0831-480c-81b1-9f45f93be53a",
   "metadata": {},
   "source": [
    "Hue screenshot here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f75ed376-d627-490e-bd90-affa5f13a2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no partitions to be shown\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark.sql(\"SHOW PARTITIONS spark.non_partitioned_table\").show()\n",
    "except:\n",
    "    print(\"There are no partitions to be shown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7425b59-61fc-4673-bf8a-3da6a04b98d7",
   "metadata": {},
   "source": [
    "Hue Screenshot here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f7ff1f-cadc-4cde-9ea1-7436337d1dcb",
   "metadata": {},
   "source": [
    "#### Partitioned Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31c90ffc-38bb-4e12-9d49-9f83088d3ac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS spark.partitioned_table\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS spark.partitioned_table\\\n",
    "    (id BIGINT, state STRING, country STRING)\\\n",
    "    USING PARQUET\\\n",
    "    PARTITIONED BY (country)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e45f6a20-2665-43a0-a8c4-3a247670081f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"INSERT INTO spark.partitioned_table VALUES (1, 'CA', 'USA'),(2, 'CA', 'USA'),\\\n",
    "                    (3, 'AZ', 'USA'),\\\n",
    "                    (4, 'ON', 'CAN'),\\\n",
    "                    (5, 'AL', 'CAN')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a1cd15-428f-4669-b52e-e46836a0d2aa",
   "metadata": {},
   "source": [
    "Hue screenshot here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e066294-28eb-4839-a8d4-c09c4a51a592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|  partition|\n",
      "+-----------+\n",
      "|country=CAN|\n",
      "|country=USA|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW PARTITIONS spark.partitioned_table\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e108eb12-2543-43e5-9267-8c4e5f279da1",
   "metadata": {},
   "source": [
    "### Spark Explain with Non-Partitioned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6abbec26-3dfd-41b9-ac85-704799a89ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_notp = spark.sql(\"SELECT * FROM spark.non_partitioned_table WHERE country='USA'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2544feef-d215-4ba0-8fd0-d7c5b49b5f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [*]\n",
      "+- 'Filter ('country = USA)\n",
      "   +- 'UnresolvedRelation [spark, non_partitioned_table], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: bigint, state: string, country: string\n",
      "Project [id#112L, state#113, country#114]\n",
      "+- Filter (country#114 = USA)\n",
      "   +- SubqueryAlias spark_catalog.spark.non_partitioned_table\n",
      "      +- HiveTableRelation [`spark`.`non_partitioned_table`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#112L, state#113, country#114], Partition Cols: []]\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Filter (isnotnull(country#114) AND (country#114 = USA))\n",
      "+- HiveTableRelation [`spark`.`non_partitioned_table`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#112L, state#113, country#114], Partition Cols: []]\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(country#114) AND (country#114 = USA))\n",
      "+- Scan hive spark.non_partitioned_table [id#112L, state#113, country#114], HiveTableRelation [`spark`.`non_partitioned_table`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#112L, state#113, country#114], Partition Cols: []]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_notp.explain(mode=\"extended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf7d4276-1766-4f8c-80cc-eb7e2aa82d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 WholeStageCodegen subtrees.\n",
      "== Subtree 1 / 1 (maxMethodCodeSize:282; maxConstantPoolSize:121(0.18% used); numInnerClasses:0) ==\n",
      "*(1) Filter (isnotnull(country#114) AND (country#114 = USA))\n",
      "+- Scan hive spark.non_partitioned_table [id#112L, state#113, country#114], HiveTableRelation [`spark`.`non_partitioned_table`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#112L, state#113, country#114], Partition Cols: []]\n",
      "\n",
      "Generated code:\n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 011 */\n",
      "/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 013 */     this.references = references;\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 017 */     partitionIndex = index;\n",
      "/* 018 */     this.inputs = inputs;\n",
      "/* 019 */     inputadapter_input_0 = inputs[0];\n",
      "/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 64);\n",
      "/* 021 */\n",
      "/* 022 */   }\n",
      "/* 023 */\n",
      "/* 024 */   protected void processNext() throws java.io.IOException {\n",
      "/* 025 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 026 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 027 */\n",
      "/* 028 */       do {\n",
      "/* 029 */         boolean inputadapter_isNull_2 = inputadapter_row_0.isNullAt(2);\n",
      "/* 030 */         UTF8String inputadapter_value_2 = inputadapter_isNull_2 ?\n",
      "/* 031 */         null : (inputadapter_row_0.getUTF8String(2));\n",
      "/* 032 */\n",
      "/* 033 */         boolean filter_value_2 = !inputadapter_isNull_2;\n",
      "/* 034 */         if (!filter_value_2) continue;\n",
      "/* 035 */\n",
      "/* 036 */         boolean filter_value_3 = false;\n",
      "/* 037 */         filter_value_3 = inputadapter_value_2.equals(((UTF8String) references[1] /* literal */));\n",
      "/* 038 */         if (!filter_value_3) continue;\n",
      "/* 039 */\n",
      "/* 040 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);\n",
      "/* 041 */\n",
      "/* 042 */         boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 043 */         long inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 044 */         -1L : (inputadapter_row_0.getLong(0));\n",
      "/* 045 */         boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n",
      "/* 046 */         UTF8String inputadapter_value_1 = inputadapter_isNull_1 ?\n",
      "/* 047 */         null : (inputadapter_row_0.getUTF8String(1));\n",
      "/* 048 */         filter_mutableStateArray_0[0].reset();\n",
      "/* 049 */\n",
      "/* 050 */         filter_mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 051 */\n",
      "/* 052 */         if (inputadapter_isNull_0) {\n",
      "/* 053 */           filter_mutableStateArray_0[0].setNullAt(0);\n",
      "/* 054 */         } else {\n",
      "/* 055 */           filter_mutableStateArray_0[0].write(0, inputadapter_value_0);\n",
      "/* 056 */         }\n",
      "/* 057 */\n",
      "/* 058 */         if (inputadapter_isNull_1) {\n",
      "/* 059 */           filter_mutableStateArray_0[0].setNullAt(1);\n",
      "/* 060 */         } else {\n",
      "/* 061 */           filter_mutableStateArray_0[0].write(1, inputadapter_value_1);\n",
      "/* 062 */         }\n",
      "/* 063 */\n",
      "/* 064 */         filter_mutableStateArray_0[0].write(2, inputadapter_value_2);\n",
      "/* 065 */         append((filter_mutableStateArray_0[0].getRow()));\n",
      "/* 066 */\n",
      "/* 067 */       } while(false);\n",
      "/* 068 */       if (shouldStop()) return;\n",
      "/* 069 */     }\n",
      "/* 070 */   }\n",
      "/* 071 */\n",
      "/* 072 */ }\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_notp.explain(mode=\"codegen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40f2a385-7375-4dfc-94a8-12a0bfca51f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Optimized Logical Plan ==\n",
      "Filter (isnotnull(country#114) AND (country#114 = USA)), Statistics(sizeInBytes=45.0 B)\n",
      "+- HiveTableRelation [`spark`.`non_partitioned_table`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#112L, state#113, country#114], Partition Cols: []], Statistics(sizeInBytes=45.0 B)\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(country#114) AND (country#114 = USA))\n",
      "+- Scan hive spark.non_partitioned_table [id#112L, state#113, country#114], HiveTableRelation [`spark`.`non_partitioned_table`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#112L, state#113, country#114], Partition Cols: []]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_notp.explain(mode=\"cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8dee5dc-c5e3-405e-b011-bb1f019639f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "* Filter (2)\n",
      "+- Scan hive spark.non_partitioned_table (1)\n",
      "\n",
      "\n",
      "(1) Scan hive spark.non_partitioned_table\n",
      "Output [3]: [id#112L, state#113, country#114]\n",
      "Arguments: [id#112L, state#113, country#114], HiveTableRelation [`spark`.`non_partitioned_table`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#112L, state#113, country#114], Partition Cols: []]\n",
      "\n",
      "(2) Filter [codegen id : 1]\n",
      "Input [3]: [id#112L, state#113, country#114]\n",
      "Condition : (isnotnull(country#114) AND (country#114 = USA))\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_notp.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ddcb911b-8f88-48c1-906e-2dfaea9baedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_notp.write.mode(\"overwrite\").parquet(\"s3a://go01-demo/lakehouse/nonpar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7028bc44-7dde-4fc0-8047-3367b05b78fa",
   "metadata": {},
   "source": [
    "Hue screenshot here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f197ef5-bca6-4290-9481-1d1627d4b7e5",
   "metadata": {},
   "source": [
    "### Spark Explain with Partitioned Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c842799-4b8d-43ba-92ff-a123d62f6a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p = spark.sql(\"SELECT * FROM spark.partitioned_table WHERE country = 'USA'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa7e976f-b425-41ca-87c5-f24a37775ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [*]\n",
      "+- 'Filter ('country = USA)\n",
      "   +- 'UnresolvedRelation [spark, partitioned_table], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: bigint, state: string, country: string\n",
      "Project [id#121L, state#122, country#123]\n",
      "+- Filter (country#123 = USA)\n",
      "   +- SubqueryAlias spark_catalog.spark.partitioned_table\n",
      "      +- Relation spark.partitioned_table[id#121L,state#122,country#123] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Filter (isnotnull(country#123) AND (country#123 = USA))\n",
      "+- Relation spark.partitioned_table[id#121L,state#122,country#123] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) ColumnarToRow\n",
      "+- FileScan parquet spark.partitioned_table[id#121L,state#122,country#123] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3a://go01-demo/warehouse/tablespace/external/hive/spark.db/partitione..., PartitionFilters: [isnotnull(country#123), (country#123 = USA)], PushedFilters: [], ReadSchema: struct<id:bigint,state:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_p.explain(mode=\"extended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e51287cf-7201-41c8-9144-dd6006d52fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 WholeStageCodegen subtrees.\n",
      "== Subtree 1 / 1 (maxMethodCodeSize:324; maxConstantPoolSize:139(0.21% used); numInnerClasses:0) ==\n",
      "*(1) ColumnarToRow\n",
      "+- FileScan parquet spark.partitioned_table[id#121L,state#122,country#123] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3a://go01-demo/warehouse/tablespace/external/hive/spark.db/partitione..., PartitionFilters: [isnotnull(country#123), (country#123 = USA)], PushedFilters: [], ReadSchema: struct<id:bigint,state:string>\n",
      "\n",
      "Generated code:\n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private int columnartorow_batchIdx_0;\n",
      "/* 010 */   private org.apache.spark.sql.execution.vectorized.OnHeapColumnVector[] columnartorow_mutableStateArray_2 = new org.apache.spark.sql.execution.vectorized.OnHeapColumnVector[3];\n",
      "/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] columnartorow_mutableStateArray_3 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 012 */   private org.apache.spark.sql.vectorized.ColumnarBatch[] columnartorow_mutableStateArray_1 = new org.apache.spark.sql.vectorized.ColumnarBatch[1];\n",
      "/* 013 */   private scala.collection.Iterator[] columnartorow_mutableStateArray_0 = new scala.collection.Iterator[1];\n",
      "/* 014 */\n",
      "/* 015 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 016 */     this.references = references;\n",
      "/* 017 */   }\n",
      "/* 018 */\n",
      "/* 019 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 020 */     partitionIndex = index;\n",
      "/* 021 */     this.inputs = inputs;\n",
      "/* 022 */     columnartorow_mutableStateArray_0[0] = inputs[0];\n",
      "/* 023 */\n",
      "/* 024 */     columnartorow_mutableStateArray_3[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 64);\n",
      "/* 025 */\n",
      "/* 026 */   }\n",
      "/* 027 */\n",
      "/* 028 */   private void columnartorow_nextBatch_0() throws java.io.IOException {\n",
      "/* 029 */     if (columnartorow_mutableStateArray_0[0].hasNext()) {\n",
      "/* 030 */       columnartorow_mutableStateArray_1[0] = (org.apache.spark.sql.vectorized.ColumnarBatch)columnartorow_mutableStateArray_0[0].next();\n",
      "/* 031 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numInputBatches */).add(1);\n",
      "/* 032 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(columnartorow_mutableStateArray_1[0].numRows());\n",
      "/* 033 */       columnartorow_batchIdx_0 = 0;\n",
      "/* 034 */       columnartorow_mutableStateArray_2[0] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(0);\n",
      "/* 035 */       columnartorow_mutableStateArray_2[1] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(1);\n",
      "/* 036 */       columnartorow_mutableStateArray_2[2] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(2);\n",
      "/* 037 */\n",
      "/* 038 */     }\n",
      "/* 039 */   }\n",
      "/* 040 */\n",
      "/* 041 */   protected void processNext() throws java.io.IOException {\n",
      "/* 042 */     if (columnartorow_mutableStateArray_1[0] == null) {\n",
      "/* 043 */       columnartorow_nextBatch_0();\n",
      "/* 044 */     }\n",
      "/* 045 */     while ( columnartorow_mutableStateArray_1[0] != null) {\n",
      "/* 046 */       int columnartorow_numRows_0 = columnartorow_mutableStateArray_1[0].numRows();\n",
      "/* 047 */       int columnartorow_localEnd_0 = columnartorow_numRows_0 - columnartorow_batchIdx_0;\n",
      "/* 048 */       for (int columnartorow_localIdx_0 = 0; columnartorow_localIdx_0 < columnartorow_localEnd_0; columnartorow_localIdx_0++) {\n",
      "/* 049 */         int columnartorow_rowIdx_0 = columnartorow_batchIdx_0 + columnartorow_localIdx_0;\n",
      "/* 050 */         boolean columnartorow_isNull_0 = columnartorow_mutableStateArray_2[0].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 051 */         long columnartorow_value_0 = columnartorow_isNull_0 ? -1L : (columnartorow_mutableStateArray_2[0].getLong(columnartorow_rowIdx_0));\n",
      "/* 052 */         boolean columnartorow_isNull_1 = columnartorow_mutableStateArray_2[1].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 053 */         UTF8String columnartorow_value_1 = columnartorow_isNull_1 ? null : (columnartorow_mutableStateArray_2[1].getUTF8String(columnartorow_rowIdx_0));\n",
      "/* 054 */         boolean columnartorow_isNull_2 = columnartorow_mutableStateArray_2[2].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 055 */         UTF8String columnartorow_value_2 = columnartorow_isNull_2 ? null : (columnartorow_mutableStateArray_2[2].getUTF8String(columnartorow_rowIdx_0));\n",
      "/* 056 */         columnartorow_mutableStateArray_3[0].reset();\n",
      "/* 057 */\n",
      "/* 058 */         columnartorow_mutableStateArray_3[0].zeroOutNullBytes();\n",
      "/* 059 */\n",
      "/* 060 */         if (columnartorow_isNull_0) {\n",
      "/* 061 */           columnartorow_mutableStateArray_3[0].setNullAt(0);\n",
      "/* 062 */         } else {\n",
      "/* 063 */           columnartorow_mutableStateArray_3[0].write(0, columnartorow_value_0);\n",
      "/* 064 */         }\n",
      "/* 065 */\n",
      "/* 066 */         if (columnartorow_isNull_1) {\n",
      "/* 067 */           columnartorow_mutableStateArray_3[0].setNullAt(1);\n",
      "/* 068 */         } else {\n",
      "/* 069 */           columnartorow_mutableStateArray_3[0].write(1, columnartorow_value_1);\n",
      "/* 070 */         }\n",
      "/* 071 */\n",
      "/* 072 */         if (columnartorow_isNull_2) {\n",
      "/* 073 */           columnartorow_mutableStateArray_3[0].setNullAt(2);\n",
      "/* 074 */         } else {\n",
      "/* 075 */           columnartorow_mutableStateArray_3[0].write(2, columnartorow_value_2);\n",
      "/* 076 */         }\n",
      "/* 077 */         append((columnartorow_mutableStateArray_3[0].getRow()));\n",
      "/* 078 */         if (shouldStop()) { columnartorow_batchIdx_0 = columnartorow_rowIdx_0 + 1; return; }\n",
      "/* 079 */       }\n",
      "/* 080 */       columnartorow_batchIdx_0 = columnartorow_numRows_0;\n",
      "/* 081 */       columnartorow_mutableStateArray_1[0] = null;\n",
      "/* 082 */       columnartorow_nextBatch_0();\n",
      "/* 083 */     }\n",
      "/* 084 */   }\n",
      "/* 085 */\n",
      "/* 086 */ }\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_p.explain(mode=\"codegen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7c5ee12-5098-436c-a033-e056daeae97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Optimized Logical Plan ==\n",
      "Filter (isnotnull(country#123) AND (country#123 = USA)), Statistics(sizeInBytes=1398.0 B)\n",
      "+- Relation spark.partitioned_table[id#121L,state#122,country#123] parquet, Statistics(sizeInBytes=1398.0 B)\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) ColumnarToRow\n",
      "+- FileScan parquet spark.partitioned_table[id#121L,state#122,country#123] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3a://go01-demo/warehouse/tablespace/external/hive/spark.db/partitione..., PartitionFilters: [isnotnull(country#123), (country#123 = USA)], PushedFilters: [], ReadSchema: struct<id:bigint,state:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_p.explain(mode=\"cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a7b6237-375b-44d0-8061-6e0e0097e264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "* ColumnarToRow (2)\n",
      "+- Scan parquet spark.partitioned_table (1)\n",
      "\n",
      "\n",
      "(1) Scan parquet spark.partitioned_table\n",
      "Output [3]: [id#121L, state#122, country#123]\n",
      "Batched: true\n",
      "Location: InMemoryFileIndex [s3a://go01-demo/warehouse/tablespace/external/hive/spark.db/partitioned_table/country=USA]\n",
      "PartitionFilters: [isnotnull(country#123), (country#123 = USA)]\n",
      "ReadSchema: struct<id:bigint,state:string>\n",
      "\n",
      "(2) ColumnarToRow [codegen id : 1]\n",
      "Input [3]: [id#121L, state#122, country#123]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_p.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb0612df-c855-4fc2-bdd1-800e94e6134b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_p.write.partitionBy(\"country\").mode(\"overwrite\").parquet(\"s3a://go01-demo/lakehouse/part\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df36f22-ec30-4bc4-a5e6-862ca9135b15",
   "metadata": {},
   "source": [
    "Hue Screenshot here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaf6516-78e6-4233-aac2-146fc7147f42",
   "metadata": {},
   "source": [
    "### Working with More Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "847955d4-03d5-435b-8600-f145edaebc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Version : VersionInfo(major='0', minor='2', patch='1', release='', build='')\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import LongType, IntegerType, StringType\n",
    "\n",
    "import dbldatagen as dg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2ddfe4e-575b-46e2-9749-6d5b35b6827d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_df(row_count = 100000, unique_vals=100000):\n",
    "\n",
    "    #shuffle_partitions_requested = 8\n",
    "    \n",
    "    #spark.conf.set(\"spark.sql.shuffle.partitions\", shuffle_partitions_requested)\n",
    "\n",
    "    country_codes = [\n",
    "        \"CN\", \"US\", \"FR\", \"CA\", \"IN\", \"JM\", \"IE\", \"PK\", \"GB\", \"IL\", \"AU\", \n",
    "        \"SG\", \"ES\", \"GE\", \"MX\", \"ET\", \"SA\", \"LB\", \"NL\", \"IT\"\n",
    "    ]\n",
    "    #country_weights = [\n",
    "    #    1300, 365, 67, 38, 1300, 3, 7, 212, 67, 9, 25, 6, 47, 83, \n",
    "    #    126, 109, 58, 8, 17,\n",
    "    #]\n",
    "\n",
    "    manufacturers = [\n",
    "        \"Delta corp\", \"Xyzzy Inc.\", \"Lakehouse Ltd\", \"Acme Corp\", \"Embanks Devices\",\n",
    "    ]\n",
    "\n",
    "    lines = [\"delta\", \"xyzzy\", \"lakehouse\", \"gadget\", \"droid\"]\n",
    "\n",
    "    testDataSpec = (\n",
    "        dg.DataGenerator(spark, name=\"device_data_set\", rows=row_count) #,partitions=partitions_num)\n",
    "        .withIdOutput()\n",
    "        # we'll use hash of the base field to generate the ids to\n",
    "        # avoid a simple incrementing sequence\n",
    "        .withColumn(\"internal_device_id\", \"long\", minValue=0x1000000000000, \n",
    "                    uniqueValues=unique_vals, omit=True, baseColumnType=\"hash\",\n",
    "        )\n",
    "        # note for format strings, we must use \"%lx\" not \"%x\" as the\n",
    "        # underlying value is a long\n",
    "        .withColumn(\n",
    "            \"device_id\", \"string\", format=\"0x%013x\", baseColumn=\"internal_device_id\"\n",
    "        )\n",
    "        # the device / user attributes will be the same for the same device id\n",
    "        # so lets use the internal device id as the base column for these attribute\n",
    "        .withColumn(\"country\", \"string\", values=country_codes, #weights=country_weights, \n",
    "                    baseColumn=\"internal_device_id\")\n",
    "        .withColumn(\"manufacturer\", \"string\", values=manufacturers, \n",
    "                    baseColumn=\"internal_device_id\", )\n",
    "        # use omit = True if you don't want a column to appear in the final output\n",
    "        # but just want to use it as part of generation of another column\n",
    "        .withColumn(\"line\", \"string\", values=lines, baseColumn=\"manufacturer\", \n",
    "                    baseColumnType=\"hash\", omit=True )\n",
    "        .withColumn(\"model_ser\", \"integer\", minValue=1, maxValue=11, baseColumn=\"device_id\", \n",
    "                    baseColumnType=\"hash\", omit=True, )\n",
    "        .withColumn(\"model_line\", \"string\", expr=\"concat(line, '#', model_ser)\", \n",
    "                    baseColumn=[\"line\", \"model_ser\"] )\n",
    "        .withColumn(\"event_type\", \"string\", \n",
    "                    values=[\"activation\", \"deactivation\", \"plan change\", \"telecoms activity\", \n",
    "                            \"internet activity\", \"device error\", ],\n",
    "                    random=True)\n",
    "        .withColumn(\"event_ts\", \"timestamp\", begin=\"2020-01-01 01:00:00\", \n",
    "                    end=\"2020-12-31 23:59:00\", \n",
    "                    interval=\"1 minute\", random=True )\n",
    "    )\n",
    "\n",
    "    dfTestData = testDataSpec.build()\n",
    "\n",
    "    display(dfTestData)\n",
    "    \n",
    "    return dfTestData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02bb842-35c1-4474-8a77-f66dd71ca06e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "214c5a97-131c-442d-8ad6-93a2d828b278",
   "metadata": {},
   "source": [
    "When you are running Spark application in yarn or any cluster manager, the default length/size of partitions RDD/DataFrame/Dataset are created with the total number of cores on all executor nodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dde082-aa79-4e76-b4a5-b12873c7279a",
   "metadata": {},
   "source": [
    "Spark automatically sets the number of “map” tasks to run on each file according to its size (though you can control it through optional parameters to SparkContext.textFile, etc), and for distributed “reduce” operations, such as groupByKey and reduceByKey, it uses the largest parent RDD’s number of partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "40cf6dfa-ca4b-4bdf-aeca-c03045e6431d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, device_id: string, country: string, manufacturer: string, model_line: string, event_type: string, event_ts: timestamp]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = generate_df(row_count = 100000, unique_vals=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab173d9a-b29f-46df-95ce-73e9449e9309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3ebb4dfa-6152-4fd5-8cf9-66d286e781a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.mode(\"overwrite\").saveAsTable('SPARK.IOT_DATA_{}'.format(username), format=\"parquet\") #partitionBy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c45c4332-b9dc-4867-8d53-99b4f27361b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3a://go01-demo/warehouse/tablespace/external/hive'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.warehouse.dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "17e7eeeb-c892-48a1-b56f-334ede1ff435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'docker.repository.cloudera.com/cloudera/cdsw/ml-runtime-jupyterlab-python3.7-standard:2022.11.1-b2'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.kubernetes.container.image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7cc951e1-b07a-42d3-b66d-6ee4ca5cb7c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o251.get.\n: java.util.NoSuchElementException: spark.driver.cores\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.noSuchElementExceptionError(QueryExecutionErrors.scala:1494)\n\tat org.apache.spark.sql.internal.SQLConf.$anonfun$getConfString$3(SQLConf.scala:4188)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.internal.SQLConf.getConfString(SQLConf.scala:4188)\n\tat org.apache.spark.sql.RuntimeConfig.get(RuntimeConfig.scala:72)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_203/2536471132.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.driver.cores\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pyspark/sql/conf.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"key\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0m_NoValue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o251.get.\n: java.util.NoSuchElementException: spark.driver.cores\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.noSuchElementExceptionError(QueryExecutionErrors.scala:1494)\n\tat org.apache.spark.sql.internal.SQLConf.$anonfun$getConfString$3(SQLConf.scala:4188)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.internal.SQLConf.getConfString(SQLConf.scala:4188)\n\tat org.apache.spark.sql.RuntimeConfig.get(RuntimeConfig.scala:72)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "spark.conf.get(\"spark.driver.cores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d62be26-70e0-4495-b008-a14dc6e7e085",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d129baba-44bc-44c0-a6aa-e3e6612a3453",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f51b679-093f-46f0-8f22-65999fa0d31d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0736484c-63e7-43c3-8c16-4cb216b8e49b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42d81a6-62d2-45fb-b13e-3562eaa67279",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8f3b0a-ed3a-4eb1-b39b-4734e3cce242",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e24c304-aa8f-4b19-9278-c1f41a9e9dd8",
   "metadata": {},
   "source": [
    "Turn AQE and DA off. Run spark jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13d3d1b-d65d-4786-bd9e-93ffb5370ff3",
   "metadata": {},
   "source": [
    "Turn DA On and do section on it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32241ee9-0979-486d-81f3-a358eb76d1be",
   "metadata": {},
   "source": [
    "Turn AQE On and do section on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ed4000-08ce-4348-83c5-32db071032ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dad39725-b9d6-47a1-8c91-de16d20041d0",
   "metadata": {},
   "source": [
    "#### Spark Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b43422-dab4-45dc-8f5b-ad50f46cd5a4",
   "metadata": {},
   "source": [
    "Spark Merge Into Workaround"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d0f8f8-5e39-4ee5-8799-88a24ddebb99",
   "metadata": {},
   "source": [
    "Spark Schema Evolution Workaround"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc7431a-5d4a-4910-9654-b19cd6aea6f5",
   "metadata": {},
   "source": [
    "Spark Partition Evolution Workaround"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208ea9e5-3405-4752-9708-d8467d479e74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
