{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "671b4d79-6642-4574-8083-e20c7566537d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19a8c1b9-f896-4980-9c8f-c2bdbc031433",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting spark.hadoop.yarn.resourcemanager.principal to pauldefusco\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"PythonSQL\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.s3guard.ddb.region\",\"us-east-2\")\\\n",
    "    .config(\"spark.yarn.access.hadoopFileSystems\",\"s3a://go01-demo\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b04531d-7e92-4e18-a241-5d8b0138c924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.eventLog.enabled', 'true'),\n",
       " ('spark.ui.proxyRedirectUri',\n",
       "  'https://spark-7hywdjpjbd7u8chr.ml-4c5feac0-3ec.go01-dem.ylcu-atmi.cloudera.site'),\n",
       " ('spark.hadoop.fs.s3a.s3guard.ddb.region', 'us-east-2'),\n",
       " ('spark.network.crypto.enabled', 'true'),\n",
       " ('spark.driver.memory', '1525m'),\n",
       " ('spark.kerberos.renewal.credentials', 'ccache'),\n",
       " ('spark.dynamicAllocation.maxExecutors', '49'),\n",
       " ('spark.eventLog.dir', 'file:///sparkeventlogs'),\n",
       " ('spark.hadoop.yarn.resourcemanager.principal', 'pauldefusco'),\n",
       " ('spark.app.id', 'spark-application-1682732794249'),\n",
       " ('spark.kubernetes.driver.annotation.cluster-autoscaler.kubernetes.io/safe-to-evict',\n",
       "  'false'),\n",
       " ('spark.ui.port', '20049'),\n",
       " ('spark.kubernetes.executor.annotation.cluster-autoscaler.kubernetes.io/safe-to-evict',\n",
       "  'false'),\n",
       " ('spark.io.encryption.enabled', 'true'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.yarn.access.hadoopFileSystems', 's3a://go01-demo'),\n",
       " ('spark.master', 'k8s://https://172.20.0.1:443'),\n",
       " ('spark.driver.bindAddress', '100.100.249.136'),\n",
       " ('spark.driver.port', '41847'),\n",
       " ('spark.kubernetes.executor.podTemplateFile', '/tmp/spark-executor.json'),\n",
       " ('spark.kubernetes.container.image',\n",
       "  'docker.repository.cloudera.com/cloudera/cdsw/ml-runtime-jupyterlab-python3.7-standard:2022.11.1-b2'),\n",
       " ('spark.dynamicAllocation.shuffleTracking.enabled', 'true'),\n",
       " ('spark.kubernetes.executor.podNamePrefix', 'cdsw-7hywdjpjbd7u8chr'),\n",
       " ('spark.kubernetes.namespace', 'mlx-user-16'),\n",
       " ('spark.kubernetes.executor.config.dir', '/var/spark/conf'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  's3a://go01-demo/warehouse/tablespace/external/hive'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.name', 'PythonSQL'),\n",
       " ('spark.yarn.rmProxy.enabled', 'false'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.app.startTime', '1682732792339'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.kubernetes.driver.pod.name', '7hywdjpjbd7u8chr'),\n",
       " ('spark.ui.allowFramingFrom',\n",
       "  'https://ml-4c5feac0-3ec.go01-dem.ylcu-atmi.cloudera.site'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.deploy.mode', 'client'),\n",
       " ('spark.driver.host', '100.100.249.136'),\n",
       " ('spark.dynamicAllocation.enabled', 'true'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.authenticate', 'true')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9724c3-c219-4d38-84b3-532cb42c0b86",
   "metadata": {},
   "source": [
    "#### Hive Metastore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabe3150-a688-459a-9829-b5e09b1bdf27",
   "metadata": {},
   "source": [
    "Spark SQL uses a Hive metastore to manage the metadata of persistent relational entities (e.g. databases, tables, columns, partitions) in a relational database (for fast access). A Hive metastore warehouse (aka spark-warehouse) is the directory where Spark SQL persists tables whereas a Hive metastore (aka metastore_db) is a relational database to manage the metadata of the persistent relational entities, e.g. databases, tables, columns, partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af97d6a-e5de-410a-bf59-64fbd5a76a82",
   "metadata": {},
   "source": [
    "#### Hive Warehouse Connector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addbd601-2428-4fc7-b0cd-f7cf237beed3",
   "metadata": {},
   "source": [
    "HWC is software for securely accessing Hive tables from Spark. You need to use the HWC if you want to access Hive managed tables from Spark. You explicitly use HWC by calling the HiveWarehouseConnector API to write to managed tables. You might use HWC without even realizing it. HWC implicitly reads tables when you run a Spark SQL query on a Hive managed table.\n",
    "\n",
    "You do not need HWC to read or write Hive external tables. You can use native Spark SQL. Spark tables will be tracked in the HMS.\n",
    "\n",
    "In this tutorial we will not use the HWC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0d0f2c0-66a3-4278-aadc-55a7ff7fb95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+\n",
      "|      catalog|namespace|\n",
      "+-------------+---------+\n",
      "|spark_catalog|  default|\n",
      "+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show catalog and database\n",
    "spark.sql(\"SHOW CURRENT NAMESPACE\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c45fb5d-7f1c-4c3d-b31e-c6727db26bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hive Session ID = ce3280e8-5184-4b22-8d58-c5b0dc7b298c\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new database\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS spark_catalog.spark\")\n",
    "spark.sql(\"USE spark_catalog.spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dec4542a-3225-495d-bdeb-f8cffa5b18b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+\n",
      "|      catalog|namespace|\n",
      "+-------------+---------+\n",
      "|spark_catalog|    spark|\n",
      "+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show catalog and database\n",
    "spark.sql(\"SHOW CURRENT NAMESPACE\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eb8a57-68ae-4325-adf8-0c50518761cd",
   "metadata": {},
   "source": [
    "#### Non Partitioned Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "985487fe-ec13-4e87-8d86-4e10a02e50ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS spark.non_partitioned_table\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS spark.non_partitioned_table\\\n",
    "            (id BIGINT, state STRING, country STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6d8446e-f121-49b8-8807-8a75554069a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"INSERT INTO spark.non_partitioned_table VALUES (1, 'CA', 'USA'),(2, 'CA', 'USA'),\\\n",
    "                    (3, 'AZ', 'USA'),\\\n",
    "                    (4, 'ON', 'CAN'),\\\n",
    "                    (5, 'AL', 'CAN')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f75ed376-d627-490e-bd90-affa5f13a2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no partitions to be shown\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark.sql(\"SHOW PARTITIONS spark.non_partitioned_table\").show()\n",
    "except:\n",
    "    print(\"There are no partitions to be shown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7425b59-61fc-4673-bf8a-3da6a04b98d7",
   "metadata": {},
   "source": [
    "Hue Screenshot here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f7ff1f-cadc-4cde-9ea1-7436337d1dcb",
   "metadata": {},
   "source": [
    "#### Partitioned Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31c90ffc-38bb-4e12-9d49-9f83088d3ac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS spark.partitioned_table\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS spark.partitioned_table\\\n",
    "    (id BIGINT, state STRING, country STRING)\\\n",
    "    USING PARQUET\\\n",
    "    PARTITIONED BY (country)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e45f6a20-2665-43a0-a8c4-3a247670081f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"INSERT INTO spark.partitioned_table VALUES (1, 'CA', 'USA'),(2, 'CA', 'USA'),\\\n",
    "                    (3, 'AZ', 'USA'),\\\n",
    "                    (4, 'ON', 'CAN'),\\\n",
    "                    (5, 'AL', 'CAN')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e066294-28eb-4839-a8d4-c09c4a51a592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|  partition|\n",
      "+-----------+\n",
      "|country=CAN|\n",
      "|country=USA|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW PARTITIONS spark.partitioned_table\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c429f30-361c-47ae-a45b-8916cf6c4df3",
   "metadata": {},
   "source": [
    "Hue Screenshot here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6abbec26-3dfd-41b9-ac85-704799a89ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_notp = spark.sql(\"SELECT * FROM spark.non_partitioned_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c842799-4b8d-43ba-92ff-a123d62f6a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p = spark.sql(\"SELECT * FROM spark.partitioned_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa7e976f-b425-41ca-87c5-f24a37775ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [*]\n",
      "+- 'UnresolvedRelation [spark, non_partitioned_table], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: bigint, state: string, country: string\n",
      "Project [id#108L, state#109, country#110]\n",
      "+- SubqueryAlias spark_catalog.spark.non_partitioned_table\n",
      "   +- HiveTableRelation [`spark`.`non_partitioned_table`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#108L, state#109, country#110], Partition Cols: []]\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "HiveTableRelation [`spark`.`non_partitioned_table`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#108L, state#109, country#110], Partition Cols: []]\n",
      "\n",
      "== Physical Plan ==\n",
      "Scan hive spark.non_partitioned_table [id#108L, state#109, country#110], HiveTableRelation [`spark`.`non_partitioned_table`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#108L, state#109, country#110], Partition Cols: []]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_notp.explain(mode=\"extended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8fbf39b-f995-439a-b1fd-39f9aea15587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 WholeStageCodegen subtrees.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_notp.explain(mode=\"codegen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31796d6e-578a-4c1b-b1fd-197578dc96cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Optimized Logical Plan ==\n",
      "HiveTableRelation [`spark`.`non_partitioned_table`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#108L, state#109, country#110], Partition Cols: []], Statistics(sizeInBytes=45.0 B)\n",
      "\n",
      "== Physical Plan ==\n",
      "Scan hive spark.non_partitioned_table [id#108L, state#109, country#110], HiveTableRelation [`spark`.`non_partitioned_table`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#108L, state#109, country#110], Partition Cols: []]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_notp.explain(mode=\"cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd2387d2-e009-48f0-b871-32c155692bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "Scan hive spark.non_partitioned_table (1)\n",
      "\n",
      "\n",
      "(1) Scan hive spark.non_partitioned_table\n",
      "Output [3]: [id#108L, state#109, country#110]\n",
      "Arguments: [id#108L, state#109, country#110], HiveTableRelation [`spark`.`non_partitioned_table`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#108L, state#109, country#110], Partition Cols: []]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_notp.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2250effc-0317-4103-86cc-1c9fd38b94e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a211b97-33b6-4f09-afbc-24135af0d08f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "026c5f9d-b982-4012-a09e-fcb91d9a5c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [*]\n",
      "+- 'UnresolvedRelation [spark, partitioned_table], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: bigint, state: string, country: string\n",
      "Project [id#114L, state#115, country#116]\n",
      "+- SubqueryAlias spark_catalog.spark.partitioned_table\n",
      "   +- Relation spark.partitioned_table[id#114L,state#115,country#116] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Relation spark.partitioned_table[id#114L,state#115,country#116] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) ColumnarToRow\n",
      "+- FileScan parquet spark.partitioned_table[id#114L,state#115,country#116] Batched: true, DataFilters: [], Format: Parquet, Location: CatalogFileIndex(1 paths)[s3a://go01-demo/warehouse/tablespace/external/hive/spark.db/partitioned..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:bigint,state:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_p.explain(mode=\"extended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e51287cf-7201-41c8-9144-dd6006d52fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 WholeStageCodegen subtrees.\n",
      "== Subtree 1 / 1 (maxMethodCodeSize:324; maxConstantPoolSize:139(0.21% used); numInnerClasses:0) ==\n",
      "*(1) ColumnarToRow\n",
      "+- FileScan parquet spark.partitioned_table[id#114L,state#115,country#116] Batched: true, DataFilters: [], Format: Parquet, Location: CatalogFileIndex(1 paths)[s3a://go01-demo/warehouse/tablespace/external/hive/spark.db/partitioned..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:bigint,state:string>\n",
      "\n",
      "Generated code:\n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private int columnartorow_batchIdx_0;\n",
      "/* 010 */   private org.apache.spark.sql.execution.vectorized.OnHeapColumnVector[] columnartorow_mutableStateArray_2 = new org.apache.spark.sql.execution.vectorized.OnHeapColumnVector[3];\n",
      "/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] columnartorow_mutableStateArray_3 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 012 */   private org.apache.spark.sql.vectorized.ColumnarBatch[] columnartorow_mutableStateArray_1 = new org.apache.spark.sql.vectorized.ColumnarBatch[1];\n",
      "/* 013 */   private scala.collection.Iterator[] columnartorow_mutableStateArray_0 = new scala.collection.Iterator[1];\n",
      "/* 014 */\n",
      "/* 015 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 016 */     this.references = references;\n",
      "/* 017 */   }\n",
      "/* 018 */\n",
      "/* 019 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 020 */     partitionIndex = index;\n",
      "/* 021 */     this.inputs = inputs;\n",
      "/* 022 */     columnartorow_mutableStateArray_0[0] = inputs[0];\n",
      "/* 023 */\n",
      "/* 024 */     columnartorow_mutableStateArray_3[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 64);\n",
      "/* 025 */\n",
      "/* 026 */   }\n",
      "/* 027 */\n",
      "/* 028 */   private void columnartorow_nextBatch_0() throws java.io.IOException {\n",
      "/* 029 */     if (columnartorow_mutableStateArray_0[0].hasNext()) {\n",
      "/* 030 */       columnartorow_mutableStateArray_1[0] = (org.apache.spark.sql.vectorized.ColumnarBatch)columnartorow_mutableStateArray_0[0].next();\n",
      "/* 031 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numInputBatches */).add(1);\n",
      "/* 032 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(columnartorow_mutableStateArray_1[0].numRows());\n",
      "/* 033 */       columnartorow_batchIdx_0 = 0;\n",
      "/* 034 */       columnartorow_mutableStateArray_2[0] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(0);\n",
      "/* 035 */       columnartorow_mutableStateArray_2[1] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(1);\n",
      "/* 036 */       columnartorow_mutableStateArray_2[2] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(2);\n",
      "/* 037 */\n",
      "/* 038 */     }\n",
      "/* 039 */   }\n",
      "/* 040 */\n",
      "/* 041 */   protected void processNext() throws java.io.IOException {\n",
      "/* 042 */     if (columnartorow_mutableStateArray_1[0] == null) {\n",
      "/* 043 */       columnartorow_nextBatch_0();\n",
      "/* 044 */     }\n",
      "/* 045 */     while ( columnartorow_mutableStateArray_1[0] != null) {\n",
      "/* 046 */       int columnartorow_numRows_0 = columnartorow_mutableStateArray_1[0].numRows();\n",
      "/* 047 */       int columnartorow_localEnd_0 = columnartorow_numRows_0 - columnartorow_batchIdx_0;\n",
      "/* 048 */       for (int columnartorow_localIdx_0 = 0; columnartorow_localIdx_0 < columnartorow_localEnd_0; columnartorow_localIdx_0++) {\n",
      "/* 049 */         int columnartorow_rowIdx_0 = columnartorow_batchIdx_0 + columnartorow_localIdx_0;\n",
      "/* 050 */         boolean columnartorow_isNull_0 = columnartorow_mutableStateArray_2[0].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 051 */         long columnartorow_value_0 = columnartorow_isNull_0 ? -1L : (columnartorow_mutableStateArray_2[0].getLong(columnartorow_rowIdx_0));\n",
      "/* 052 */         boolean columnartorow_isNull_1 = columnartorow_mutableStateArray_2[1].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 053 */         UTF8String columnartorow_value_1 = columnartorow_isNull_1 ? null : (columnartorow_mutableStateArray_2[1].getUTF8String(columnartorow_rowIdx_0));\n",
      "/* 054 */         boolean columnartorow_isNull_2 = columnartorow_mutableStateArray_2[2].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 055 */         UTF8String columnartorow_value_2 = columnartorow_isNull_2 ? null : (columnartorow_mutableStateArray_2[2].getUTF8String(columnartorow_rowIdx_0));\n",
      "/* 056 */         columnartorow_mutableStateArray_3[0].reset();\n",
      "/* 057 */\n",
      "/* 058 */         columnartorow_mutableStateArray_3[0].zeroOutNullBytes();\n",
      "/* 059 */\n",
      "/* 060 */         if (columnartorow_isNull_0) {\n",
      "/* 061 */           columnartorow_mutableStateArray_3[0].setNullAt(0);\n",
      "/* 062 */         } else {\n",
      "/* 063 */           columnartorow_mutableStateArray_3[0].write(0, columnartorow_value_0);\n",
      "/* 064 */         }\n",
      "/* 065 */\n",
      "/* 066 */         if (columnartorow_isNull_1) {\n",
      "/* 067 */           columnartorow_mutableStateArray_3[0].setNullAt(1);\n",
      "/* 068 */         } else {\n",
      "/* 069 */           columnartorow_mutableStateArray_3[0].write(1, columnartorow_value_1);\n",
      "/* 070 */         }\n",
      "/* 071 */\n",
      "/* 072 */         if (columnartorow_isNull_2) {\n",
      "/* 073 */           columnartorow_mutableStateArray_3[0].setNullAt(2);\n",
      "/* 074 */         } else {\n",
      "/* 075 */           columnartorow_mutableStateArray_3[0].write(2, columnartorow_value_2);\n",
      "/* 076 */         }\n",
      "/* 077 */         append((columnartorow_mutableStateArray_3[0].getRow()));\n",
      "/* 078 */         if (shouldStop()) { columnartorow_batchIdx_0 = columnartorow_rowIdx_0 + 1; return; }\n",
      "/* 079 */       }\n",
      "/* 080 */       columnartorow_batchIdx_0 = columnartorow_numRows_0;\n",
      "/* 081 */       columnartorow_mutableStateArray_1[0] = null;\n",
      "/* 082 */       columnartorow_nextBatch_0();\n",
      "/* 083 */     }\n",
      "/* 084 */   }\n",
      "/* 085 */\n",
      "/* 086 */ }\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_p.explain(mode=\"codegen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7c5ee12-5098-436c-a033-e056daeae97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Optimized Logical Plan ==\n",
      "Relation spark.partitioned_table[id#114L,state#115,country#116] parquet, Statistics(sizeInBytes=8.0 EiB)\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) ColumnarToRow\n",
      "+- FileScan parquet spark.partitioned_table[id#114L,state#115,country#116] Batched: true, DataFilters: [], Format: Parquet, Location: CatalogFileIndex(1 paths)[s3a://go01-demo/warehouse/tablespace/external/hive/spark.db/partitioned..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:bigint,state:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_p.explain(mode=\"cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a7b6237-375b-44d0-8061-6e0e0097e264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "* ColumnarToRow (2)\n",
      "+- Scan parquet spark.partitioned_table (1)\n",
      "\n",
      "\n",
      "(1) Scan parquet spark.partitioned_table\n",
      "Output [3]: [id#114L, state#115, country#116]\n",
      "Batched: true\n",
      "Location: CatalogFileIndex [s3a://go01-demo/warehouse/tablespace/external/hive/spark.db/partitioned_table]\n",
      "ReadSchema: struct<id:bigint,state:string>\n",
      "\n",
      "(2) ColumnarToRow [codegen id : 1]\n",
      "Input [3]: [id#114L, state#115, country#116]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_p.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba312b0a-2eeb-4830-910e-e419e52e0749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3baafc8-92b9-4869-97fb-9666e7359c28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ed4000-08ce-4348-83c5-32db071032ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dad39725-b9d6-47a1-8c91-de16d20041d0",
   "metadata": {},
   "source": [
    "#### Spark Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b43422-dab4-45dc-8f5b-ad50f46cd5a4",
   "metadata": {},
   "source": [
    "Spark Merge Into Workaround"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d0f8f8-5e39-4ee5-8799-88a24ddebb99",
   "metadata": {},
   "source": [
    "Spark Schema Evolution Workaround"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc7431a-5d4a-4910-9654-b19cd6aea6f5",
   "metadata": {},
   "source": [
    "Spark Partition Evolution Workaround"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208ea9e5-3405-4752-9708-d8467d479e74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
